{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b564b94-05af-475a-8137-5950ea1eafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "#from depth_anything.dpt import DepthAnything\n",
    "#from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4cb69f-0bb6-40da-b196-c66f5bced5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoedepth.utils.easydict import EasyDict as edict\n",
    "#from zoedepth.data.data_mono import DepthDataLoader\n",
    "from zoedepth.models.builder import build_model\n",
    "from zoedepth.utils.arg_utils import parse_unknown\n",
    "from zoedepth.utils.config import change_dataset, get_config, ALL_EVAL_DATASETS, ALL_INDOOR, ALL_OUTDOOR\n",
    "from zoedepth.utils.misc import (RunningAverageDict, colors, compute_metrics,count_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fdc222-99ee-4140-8d2b-199066cae650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45638dd-e79f-4d54-b57f-7e65ef6a8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resource=\"local::./checkpoints/depth_anything_metric_depth_outdoor.pt\"\n",
    "m_name = \"zoedepth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69ef8fa-cf64-4efa-9c80-bbdbbd17b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'nyu', 'avoid_boundary': False, 'min_depth': 0.001, 'max_depth': 10, 'data_path': './data/nyu', 'gt_path': './data/nyu', 'filenames_file': './train_test_inputs/nyudepthv2_train_files_with_gt.txt', 'input_height': 480, 'input_width': 640, 'data_path_eval': './data/nyu', 'gt_path_eval': './data/nyu', 'filenames_file_eval': './train_test_inputs/nyudepthv2_test_files_with_gt.txt', 'min_depth_eval': 0.001, 'max_depth_eval': 10, 'min_depth_diff': -10, 'max_depth_diff': 10, 'do_random_rotate': True, 'degree': 1.0, 'do_kb_crop': False, 'garg_crop': False, 'eigen_crop': True, 'save_dir': './depth_anything_finetune', 'project': 'ZoeDepth', 'tags': '', 'notes': '', 'gpu': None, 'root': '.', 'uid': None, 'print_losses': False, 'distributed': True, 'workers': 16, 'clip_grad': 0.1, 'use_shared_dict': False, 'shared_dict': None, 'use_amp': False, 'aug': True, 'random_crop': False, 'random_translate': False, 'translate_prob': 0.2, 'max_translation': 100, 'validate_every': 0.25, 'log_images_every': 0.1, 'prefetch': False, 'name': 'ZoeDepth', 'version_name': 'v1', 'n_bins': 64, 'bin_embedding_dim': 128, 'bin_centers_type': 'softplus', 'n_attractors': [16, 8, 4, 1], 'attractor_alpha': 1000, 'attractor_gamma': 2, 'attractor_kind': 'mean', 'attractor_type': 'inv', 'midas_model_type': 'DPT_BEiT_L_384', 'min_temp': 0.0212, 'max_temp': 50.0, 'output_distribution': 'logbinomial', 'memory_efficient': True, 'inverse_midas': False, 'img_size': [392, 518], 'train_midas': False, 'use_pretrained_midas': False, 'pretrained_resource': 'local::./checkpoints/depth_anything_metric_depth_outdoor.pt', 'model': 'zoedepth'}\n"
     ]
    }
   ],
   "source": [
    "overwrite = {\"pretrained_resource\": pretrained_resource}\n",
    "config = get_config(m_name, \"eval\", 'nyu', **overwrite)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612e80e2-e7f9-4d19-af48-60d4013b1a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  518\n",
      "\theight:  392\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  False\n",
      "\tensure_multiple_of:  14\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource local::./checkpoints/depth_anything_metric_depth_outdoor.pt\n",
      "Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model = build_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b7c7780-6575-40b4-ae1d-ad1dd92d76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pose_file = \"/home/jason/ROS/bags/analysis/pose.txt\"\n",
    "img_pose = list()\n",
    "\n",
    "f = open(img_pose_file, \"r\")\n",
    "content = f.readlines()\n",
    "\n",
    "frame1 = content[75].split(',')\n",
    "frame2 = content[76].split(',')\n",
    "\n",
    "frame1_pose = (float(frame1[1]), float(frame1[2]), float(frame1[3]))\n",
    "frame1_img = frame1[0]\n",
    "\n",
    "frame2_pose = (float(frame2[1]), float(frame2[2]), float(frame2[3]))\n",
    "frame2_img = frame2[0]\n",
    "\n",
    "img1 = read_image(\"/home/jason/ROS/bags/images/wp_ground/\"+frame1_img, mode=ImageReadMode.RGB)\n",
    "img2 = read_image(\"/home/jason/ROS/bags/images/wp_ground/\"+frame2_img, mode=ImageReadMode.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99dad828-9a61-45f3-8763-ed3d5d332b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZoeDepth(\n",
       "  (core): DepthAnythingCore(\n",
       "    (core): DPT_DINOv2(\n",
       "      (pretrained): DinoVisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0-23): 24 x NestedTensorBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (head): Identity()\n",
       "      )\n",
       "      (depth_head): DPTHead(\n",
       "        (projects): ModuleList(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (resize_layers): ModuleList(\n",
       "          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (scratch): Module(\n",
       "          (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (refinenet1): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet2): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet3): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet4): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (output_conv2): Sequential(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (seed_bin_regressor): SeedBinRegressorUnnormed(\n",
       "    (_net): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       "  (seed_projector): Projector(\n",
       "    (_net): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (projectors): ModuleList(\n",
       "    (0-3): 4 x Projector(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attractors): ModuleList(\n",
       "    (0): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "    (1): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "    (2): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "    (3): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conditional_log_binomial): ConditionalLogBinomial(\n",
       "    (log_binomial_transform): LogBinomial()\n",
       "    (mlp): Sequential(\n",
       "      (0): Conv2d(161, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv2d(80, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60dc812-11bb-4411-800d-41a21df50b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, img, **kwargs):\n",
    "    pred = model(img, **kwargs)\n",
    "    return get_depth_from_prediction(pred)\n",
    "\n",
    "def get_depth_from_prediction(pred):\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        pred = pred  # pass\n",
    "    elif isinstance(pred, (list, tuple)):\n",
    "        pred = pred[-1]\n",
    "    elif isinstance(pred, dict):\n",
    "        pred = pred['metric_depth'] if 'metric_depth' in pred else pred['out']\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown output type {type(pred)}\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ecb100-5cd9-4686-a369-84f2d527d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 720, 1280]) torch.uint8\n"
     ]
    }
   ],
   "source": [
    "focal = torch.Tensor([(879.2263626388094 + 879.3081232332211) / 2])\n",
    "\n",
    "img_t = img1.unsqueeze(0)\n",
    "print(img_t.shape, img1.dtype)\n",
    "img_t = img_t.type(torch.float)\n",
    "\n",
    "pred = infer(model.cuda(), img_t.cuda(), focal=focal.cuda() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c02e718-644d-4d78-badc-d21c62c1507c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [518] and output size of (720, 1280). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m720\u001b[39m\n\u001b[1;32m      5\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1280\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m d_map \u001b[38;5;241m=\u001b[39m colorize(depth\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(depth\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/torch/lib/python3.11/site-packages/torch/nn/functional.py:3916\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   3915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 3916\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3917\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3918\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3921\u001b[0m         )\n\u001b[1;32m   3922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   3923\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [518] and output size of (720, 1280). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "from zoedepth.utils.misc import colorize\n",
    "\n",
    "depth = pred.squeeze(0,1).cpu()\n",
    "\n",
    "d_map = colorize(depth.detach().numpy(), 0, 10)\n",
    "plt.imshow(depth.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3f248-847a-44c9-822e-9a06365dbed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
